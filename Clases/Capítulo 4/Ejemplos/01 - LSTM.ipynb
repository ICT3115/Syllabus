{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "01 - LSTM_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiA6v1kb9zRv"
      },
      "source": [
        "# Redes recurrentes para datos secuenciales\n",
        "En este ejemplo, utilizaremos redes recurrentes (LSTM) para aprender un modelo de seguimiento vehicular. Definiéndolos de manera muy general, resumida y simplista, estos modelos buscan capturar el comportamiento de un conductor cuando se encuentra manejando detras de otro vehículo. Para este caso particular, utilizaremos datos capturados en el experimento BLABLABLA, que nos entrega una secuencia de mediciones para cada uno de los vehículos participantes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-sIfQ-2GGkE"
      },
      "source": [
        "## 1. Importación de librerías\n",
        "La importación es muy similar a los ejemplos que hemos visto anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ8XoZOEVGia"
      },
      "source": [
        "!pip install hiddenlayer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAh4UEe9HSfU"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import hiddenlayer as hl\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "import time\n",
        "#import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NFzbNnhGaYo"
      },
      "source": [
        "## 2. Creación del set de datos\n",
        "A diferencia de los casos anteriores, en este ejemplo crearemos una clase que además de entregar las facilidades de un `Dataset` que hemos visto antes, preprocese los datos para dejarlos listos para ser usados. El preprocesamiento se realiza en el método `create_sequences`, y recibe como parámetro un `DataFrame` de Pandas creado a partir de un archivo .csv.\n",
        "\n",
        "Los datos se encuentran almacenados en el archivo `data.csv` en formato tabular, donde para cada vehículo se muestrean datos 30 veces por segundo. \n",
        "\n",
        "En particular, para cada observación (par [vehículo, instante de tiempo]) tendremos las variables `accel` (aceleración del vehículo), `speed` (velocidad del vehículo), `lead_speed` (velocidad del vehículo que se está siguiendo), y `gap` (tiempo transcurrido entre que el fin (parachoque trasero) del vehículo seguido y el inicio (parachoque delantero) del vehículo modelado pasan por un mismo punto). Como features (tensor `X`) utilizaremos `speed`, `lead_speed` y `gap`, mientras que como variable objetivo (vector `Y`) utilizaremos `accel`. \n",
        "\n",
        "Finalmente, y con el fin de tener los datos en el formato adecuado para una red recurrente, transformaremos la tabla de datos en un arreglo tridimensional con formato [obs,time_step,features], donde cada observación será ahora una secuencia de 2 _time steps_. Dado que originalmente cada fila almacena las variables para un par (vehículo, tiempo), para crear las secuencias concatenaremos cada fila con la fila inmediatamente siguiente y luego realizaremos un `reshape` del tensor para llevarlo de 2 a 3 dimensiones (los datos son los mismos, solo se reorganizan)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNMUBKiqUPdI"
      },
      "source": [
        "class RingDataset(Dataset):\n",
        "  def __init__(self, path):\n",
        "    self.data, self.X, self.Y = self.create_sequences(pd.read_csv(path, compression='xz'))\n",
        "    self.X = torch.tensor(self.X)\n",
        "    self.Y = torch.tensor(self.Y)\n",
        "      \n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.Y[idx]\n",
        "  \n",
        "  @staticmethod\n",
        "  def create_sequences(data):\n",
        "    # Transformamos los datos desde un arreglo con dos dimensiones [obs, features], \n",
        "    # a un arreglo tridimensional [obs,time_step,features]\n",
        "    \n",
        "    NUM_FEATURES = 3\n",
        "    TIME_STEP = 2\n",
        "\n",
        "    data = pd.concat([data[['speed', 'lead_speed', 'gap']], \n",
        "                      data[['speed', 'lead_speed', 'gap', 'accel', 'time']].shift(-1)], axis=1)             \n",
        "    data.dropna(inplace=True)\n",
        "    \n",
        "    Y = data['accel'].values.astype('float32')\n",
        "    X = data.drop(['accel', 'time'], axis=1).values\n",
        "    X = X.reshape(X.shape[0], TIME_STEP, NUM_FEATURES).astype('float32')\n",
        "\n",
        "    return data, X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_5E9GhFXXBQ"
      },
      "source": [
        "## 3. Creación del modelo\n",
        "En este paso crearemos la red, utilizando inicialmente dos capas de LSTM, donde cada una de estas es seguida de _dropout_ con probabilidad de 0.2. Luego de esto, tenemos una capa lineal final, que produce la predicción de la aceleración. Queda como modificación propuesta ver el efecto de cambiar el dropout y de agregar capas de _batch normalization_.\n",
        "\n",
        "Un detalle relevante es que en Pytorch las LSTM retornan la salida para cada paso de la secuencia (no solo para el final). Dado que en este caso solo nos interesa generar la predicción una vez que se procesa el último elemento de la secuencia, en el forward solo rescatamos la última columna del tensor `x` (`x[:,-1]`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHVjj8KeIrEE"
      },
      "source": [
        "class TrafficNet(nn.Module):\n",
        "  def __init__(self, input_size = 3, hidden_units = 128):\n",
        "    super(TrafficNet, self).__init__()\n",
        "    self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_units, \n",
        "                        num_layers = 2, batch_first = True, dropout = 0.2)\n",
        "    self.dropout = nn.Dropout(p=0.2)\n",
        "    self.linear = nn.Linear(hidden_units, 1)\n",
        "      \n",
        "  def forward(self, x):\n",
        "    x, _ = self.lstm(x)\n",
        "    x = self.dropout(x[:,-1])\n",
        "    x = self.linear(x)\n",
        "    return x.squeeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSVRSNEd84Mk"
      },
      "source": [
        "## 4. Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcsClNWSUPdJ"
      },
      "source": [
        "training_set = RingDataset(\"data.csv\")\n",
        "params = {'batch_size': 2048,\n",
        "          'num_workers': 1,\n",
        "          'pin_memory': True}\n",
        "data_loader = DataLoader(training_set, **params, shuffle = True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = TrafficNet().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPMv278YZ9gl"
      },
      "source": [
        "## 5. Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBtXMjMIUPdJ"
      },
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 50\n",
        "\n",
        "since = time.time()\n",
        "history = hl.History()\n",
        "history_train = hl.History()\n",
        "canvas_train = hl.Canvas()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  start_epoch = time.time()\n",
        "  epoch_loss = 0.0\n",
        "  with tqdm.notebook.tqdm(total=len(data_loader), unit='batch', \n",
        "                          desc=f'Epoch {epoch+1}/{num_epochs}', position=100, leave=True) as pbar:                \n",
        "    for i, (X, Y) in enumerate(data_loader):\n",
        "      X = X.to(device)\n",
        "      Y = Y.to(device)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      Y_ = model(X) \n",
        "      loss = criterion(Y_, Y)                        \n",
        "      loss.backward()                                \n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_loss = (epoch_loss*i + loss.item())/(i+1)\n",
        "      pbar.set_postfix(loss=epoch_loss)\n",
        "      pbar.update()             \n",
        "  epoch_elapsed = time.time() - start_epoch\n",
        "  history_train.log(epoch, loss = epoch_loss)\n",
        "  \n",
        "time_elapsed = time.time() - since\n",
        "print('Training complete in {:.0f}m'.format(time_elapsed/60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNhQQcQ__Pai"
      },
      "source": [
        "# Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoi1aNZDUPdK"
      },
      "source": [
        "canvas =  hl.Canvas()\n",
        "with canvas:\n",
        "    canvas.draw_plot(history_train[\"loss\"], labels=['train loss'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}